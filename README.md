# Volume-Gesture
 Controlling volume using gesture 

ðŸ”¥ Tech Stack:
TensorFlow & Keras: Leveraged the MobileNetV2 architecture for transfer learning to extract features and built custom fully connected layers for gesture classification.
OpenCV: Integrated for real-time image processing and gesture detection.
Pycaw: Used to interface with system audio controls, enabling volume adjustments based on gesture predictions.
Python Libraries: Employed libraries like NumPy for data manipulation and Scikit-learn for performance evaluation.

âœ¨ Key Steps:
Data Preparation: Loaded and preprocessed images of hand gestures.
Model Development: Utilized MobileNetV2 for feature extraction, adding custom layers for classification.
Training: Compiled and trained the model with augmented data using the Adam optimizer.
Deployment: Integrated the model with OpenCV for real-time gesture detection and Pycaw for controlling system volume.

This project showcases the power of AI and machine learning in enhancing human-computer interaction, providing an intuitive, hands-free control method. ðŸ’¡
